{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93df167-3a4f-434a-b309-8573a1ecc645",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93eafa9-abcd-4202-ac0a-4ed477c2fccc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-Profiling\n",
      "  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ydata-profiling (from pandas-Profiling)\n",
      "  Downloading ydata_profiling-4.6.2-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy<1.12,>=1.4.1 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (1.11.3)\n",
      "Collecting pandas!=1.4.0,<2.1,>1.1 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting matplotlib<=3.7.3,>=3.2 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading matplotlib-3.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting pydantic>=2 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (6.0.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (3.1.2)\n",
      "Collecting visions==0.7.5 (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-Profiling)\n",
      "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.26,>=1.16.0 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting htmlmin==0.1.12 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting phik<0.13,>=0.11.1 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading phik-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.5/679.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.24.0 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (4.66.1)\n",
      "Collecting seaborn<0.13,>=0.10.1 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m617.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multimethod<2,>=1.4 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading multimethod-1.10-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (0.14.0)\n",
      "Collecting typeguard<5,>=4.1.2 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading typeguard-4.1.5-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting imagehash==4.3.1 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m592.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wordcloud>=1.9.1 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading wordcloud-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dacite>=1.8 (from ydata-profiling->pandas-Profiling)\n",
      "  Downloading dacite-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numba<0.59.0,>=0.56.0 in /opt/mamba/lib/python3.10/site-packages (from ydata-profiling->pandas-Profiling) (0.58.1)\n",
      "Collecting PyWavelets (from imagehash==4.3.1->ydata-profiling->pandas-Profiling)\n",
      "  Downloading pywavelets-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling->pandas-Profiling) (10.1.0)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /opt/mamba/lib/python3.10/site-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling->pandas-Profiling) (23.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /opt/mamba/lib/python3.10/site-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling->pandas-Profiling) (3.2.1)\n",
      "Collecting tangled-up-in-unicode>=0.0.4 (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling->pandas-Profiling)\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h\u001b[33mWARNING: visions 0.7.5 does not provide the extra 'type-image-path'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-Profiling) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.10/site-packages (from matplotlib<=3.7.3,>=3.2->ydata-profiling->pandas-Profiling) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/mamba/lib/python3.10/site-packages (from numba<0.59.0,>=0.56.0->ydata-profiling->pandas-Profiling) (0.41.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.10/site-packages (from pandas!=1.4.0,<2.1,>1.1->ydata-profiling->pandas-Profiling) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/mamba/lib/python3.10/site-packages (from pandas!=1.4.0,<2.1,>1.1->ydata-profiling->pandas-Profiling) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /opt/mamba/lib/python3.10/site-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-Profiling) (1.3.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2->ydata-profiling->pandas-Profiling)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic>=2->ydata-profiling->pandas-Profiling)\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/mamba/lib/python3.10/site-packages (from pydantic>=2->ydata-profiling->pandas-Profiling) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-Profiling) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-Profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-Profiling) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-Profiling) (2023.11.17)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/mamba/lib/python3.10/site-packages (from statsmodels<1,>=0.13.2->ydata-profiling->pandas-Profiling) (0.5.3)\n",
      "Requirement already satisfied: six in /opt/mamba/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata-profiling->pandas-Profiling) (1.16.0)\n",
      "Downloading ydata_profiling-4.6.2-py2.py3-none-any.whl (357 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m844.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
      "Downloading matplotlib-3.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n",
      "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
      "Downloading wordcloud-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.4/455.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading pywavelets-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27082 sha256=77dce8073ea5edd6abbf126726968d94cc4e501e15dc583d4ce4e3f9165087fc\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/dd/91/29/a79cecb328d01739e64017b6fb9a1ab9d8cb1853098ec5966d\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, typeguard, tangled-up-in-unicode, pydantic-core, numpy, multimethod, dacite, annotated-types, PyWavelets, pydantic, pandas, visions, matplotlib, imagehash, wordcloud, seaborn, phik, ydata-profiling, pandas-Profiling\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.3\n",
      "    Uninstalling pandas-2.1.3:\n",
      "      Successfully uninstalled pandas-2.1.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.1\n",
      "    Uninstalling matplotlib-3.8.1:\n",
      "      Successfully uninstalled matplotlib-3.8.1\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.13.0\n",
      "    Uninstalling seaborn-0.13.0:\n",
      "      Successfully uninstalled seaborn-0.13.0\n",
      "Successfully installed PyWavelets-1.5.0 annotated-types-0.6.0 dacite-1.8.1 htmlmin-0.1.12 imagehash-4.3.1 matplotlib-3.7.3 multimethod-1.10 numpy-1.25.2 pandas-2.0.3 pandas-Profiling-3.6.6 phik-0.12.3 pydantic-2.5.2 pydantic-core-2.14.5 seaborn-0.12.2 tangled-up-in-unicode-0.2.0 typeguard-4.1.5 visions-0.7.5 wordcloud-1.9.2 ydata-profiling-4.6.2\n"
     ]
    }
   ],
   "source": [
    "! pip install -q pandas-Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4b52b31-33d5-4fb9-84fb-0d2d80123de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from io import StringIO\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be42b2c-2388-4fc5-a08c-089c1fad2385",
   "metadata": {},
   "source": [
    "# Importation des bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa772fd-24f1-44f8-b14a-bc0e7c888e86",
   "metadata": {},
   "source": [
    "## Import de la base de données de consommation d'électricité  et des données météorologiques via API de l'OpenDataSoft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626348b-c5f4-498d-9b7c-f5dca7e0c69b",
   "metadata": {},
   "source": [
    "L'éxécution de la requête API prend assez de temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faeb8ad9-b84f-4707-8730-cdc94fa6e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ODS(dataset_name):\n",
    "    \"\"\"\n",
    "    Fonction qui permet de récupérer des données via l'API d'OpenDataSoft\n",
    "    Elle retourne un dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # URL de base pour accéder à l'API d'ODS\n",
    "    base_url_ODS = \"https://odre.opendatasoft.com/api/explore/v2.1\"\n",
    "    \n",
    "    dataset_path = f\"/catalog/datasets/{dataset_name}/exports/json?lang=fr&timezone=Europe%2FBerlin\"\n",
    "    url = f\"{base_url_ODS}{dataset_path}\"\n",
    "    \n",
    "    # Exécution de la requête GET\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    # Vérification si la requête a réussi\n",
    "    if response.status_code == 200:\n",
    "        # Extraction des données\n",
    "        data = response.json()\n",
    "        # Conversion des résultats en DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        print(f\"Erreur lors de la requête: {response.status_code}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data_ODSW(dataset_name, output = \"json\"):\n",
    "    \"\"\"\n",
    "    Fonction qui permet de récupérer des données via l'API  publique d'OpenDataSoft\n",
    "    Elle retourne un dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # URL de base pour accéder à l'API d'ODS\n",
    "    base_url_ODS = \"https://public.opendatasoft.com/api/explore/v2.1\"\n",
    "    \n",
    "    dataset_path = f\"/catalog/datasets/{dataset_name}/exports/{output}?lang=fr&timezone=Europe%2FBerlin\"\n",
    "    url = f\"{base_url_ODS}{dataset_path}\"\n",
    "    \n",
    "    # Exécution de la requête GET\n",
    "    response = requests.get(url)\n",
    "    # Vérification si la requête a réussi\n",
    "    if response.status_code == 200:\n",
    "        # Extraction des données\n",
    "        if output == \"json\":\n",
    "            data = response.json()\n",
    "            # Conversion des résultats en DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            data = response.json()\n",
    "            geometries = [shape(feature['geometry']) for feature in data['features']]\n",
    "            properties = [feature['properties'] for feature in data['features']]\n",
    "    \n",
    "            # Créer un GeoDataFrame en combinant les géométries et les propriétés\n",
    "            df = gpd.GeoDataFrame(geometry=geometries, data=properties)\n",
    "    else:\n",
    "        print(f\"Erreur lors de la requête: {response.status_code}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8d74c6-f322-441a-a073-68fc6f65b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la requête: 404\n"
     ]
    }
   ],
   "source": [
    "consumption_dataset_name = \"consommation-quotidienne-brute-regionale\"\n",
    "weather_dataset_name = \"donnees-synop-essentielles-omm\"\n",
    "geographic_dataset_name = \"georef-france-commune\"\n",
    "data_consumption = get_data_ODS(consumption_dataset_name)\n",
    "data_weather = get_data_ODSW(weather_dataset_name)\n",
    "data_geo = get_data_ODSW(geographic_dataset_name, output=\"geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72698e51-f39d-44d7-894b-1ef62abed711",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numer_sta</th>\n",
       "      <th>date</th>\n",
       "      <th>pmer</th>\n",
       "      <th>tend</th>\n",
       "      <th>cod_tend</th>\n",
       "      <th>dd</th>\n",
       "      <th>ff</th>\n",
       "      <th>t</th>\n",
       "      <th>td</th>\n",
       "      <th>u</th>\n",
       "      <th>...</th>\n",
       "      <th>altitude</th>\n",
       "      <th>libgeo</th>\n",
       "      <th>codegeo</th>\n",
       "      <th>nom_epci</th>\n",
       "      <th>code_epci</th>\n",
       "      <th>nom_dept</th>\n",
       "      <th>code_dep</th>\n",
       "      <th>nom_reg</th>\n",
       "      <th>code_reg</th>\n",
       "      <th>mois_de_l_annee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07558</td>\n",
       "      <td>2010-01-05T10:00:00+01:00</td>\n",
       "      <td>100280.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>260.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>275.75</td>\n",
       "      <td>275.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>712</td>\n",
       "      <td>Millau</td>\n",
       "      <td>12145</td>\n",
       "      <td>CC de Millau Grands Causses</td>\n",
       "      <td>241200567</td>\n",
       "      <td>Aveyron</td>\n",
       "      <td>12</td>\n",
       "      <td>Occitanie</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61976</td>\n",
       "      <td>2010-01-05T10:00:00+01:00</td>\n",
       "      <td>100990.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>305.45</td>\n",
       "      <td>299.05</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07027</td>\n",
       "      <td>2010-01-05T13:00:00+01:00</td>\n",
       "      <td>100720.0</td>\n",
       "      <td>-190.0</td>\n",
       "      <td>8</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>273.65</td>\n",
       "      <td>271.75</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>Carpiquet</td>\n",
       "      <td>14137</td>\n",
       "      <td>CU Caen la Mer</td>\n",
       "      <td>200065597</td>\n",
       "      <td>Calvados</td>\n",
       "      <td>14</td>\n",
       "      <td>Normandie</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07110</td>\n",
       "      <td>2010-01-05T13:00:00+01:00</td>\n",
       "      <td>100750.0</td>\n",
       "      <td>-230.0</td>\n",
       "      <td>8</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>276.95</td>\n",
       "      <td>272.55</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>Guipavas</td>\n",
       "      <td>29075</td>\n",
       "      <td>Brest Métropole</td>\n",
       "      <td>242900314</td>\n",
       "      <td>Finistère</td>\n",
       "      <td>29</td>\n",
       "      <td>Bretagne</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07591</td>\n",
       "      <td>2010-01-05T13:00:00+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.45</td>\n",
       "      <td>269.05</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>871</td>\n",
       "      <td>Embrun</td>\n",
       "      <td>05046</td>\n",
       "      <td>CC Serre-Ponçon</td>\n",
       "      <td>200067742</td>\n",
       "      <td>Hautes-Alpes</td>\n",
       "      <td>05</td>\n",
       "      <td>Provence-Alpes-Côte d'Azur</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  numer_sta                       date      pmer   tend cod_tend     dd   ff  \\\n",
       "0     07558  2010-01-05T10:00:00+01:00  100280.0  -50.0        5  260.0  1.5   \n",
       "1     61976  2010-01-05T10:00:00+01:00  100990.0    NaN     None    NaN  NaN   \n",
       "2     07027  2010-01-05T13:00:00+01:00  100720.0 -190.0        8  200.0  3.6   \n",
       "3     07110  2010-01-05T13:00:00+01:00  100750.0 -230.0        8  210.0  4.1   \n",
       "4     07591  2010-01-05T13:00:00+01:00       NaN    NaN     None    NaN  NaN   \n",
       "\n",
       "        t      td      u  ...  altitude     libgeo codegeo  \\\n",
       "0  275.75  275.75  100.0  ...       712     Millau   12145   \n",
       "1  305.45  299.05   69.0  ...         7       None    None   \n",
       "2  273.65  271.75   87.0  ...        67  Carpiquet   14137   \n",
       "3  276.95  272.55   73.0  ...        94   Guipavas   29075   \n",
       "4  274.45  269.05   67.0  ...       871     Embrun   05046   \n",
       "\n",
       "                      nom_epci  code_epci      nom_dept  code_dep  \\\n",
       "0  CC de Millau Grands Causses  241200567       Aveyron        12   \n",
       "1                         None       None          None      None   \n",
       "2               CU Caen la Mer  200065597      Calvados        14   \n",
       "3              Brest Métropole  242900314     Finistère        29   \n",
       "4              CC Serre-Ponçon  200067742  Hautes-Alpes        05   \n",
       "\n",
       "                      nom_reg code_reg mois_de_l_annee  \n",
       "0                   Occitanie       76               1  \n",
       "1                        None     None               1  \n",
       "2                   Normandie       28               1  \n",
       "3                    Bretagne       53               1  \n",
       "4  Provence-Alpes-Côte d'Azur       93               1  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "141686b4-b2b5-437c-8f54-091c57ebb1b2",
   "metadata": {},
   "source": [
    "## Import d'une base de données complémentaire (celle des régions, communes, et départements de France"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536b75e-663d-455e-b2e2-256c720fcfeb",
   "metadata": {},
   "source": [
    "Cette importation va se faire à travers une requête via l'API de Datagouv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aeb4c0a-3957-4c20-af9d-b1ee19b5ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifiant du dataset\n",
    "dataset_id = \"4a7c837bb6da8e363604082bcc8b2e504cf08038\"\n",
    "dataset_sha1 = \"dbe8a621-a9c4-4bc3-9cae-be1699c5ff25\"\n",
    "\n",
    "# URL de base pour accéder à l'API\n",
    "base_url = \"https://www.data.gouv.fr/api/1/\"\n",
    "\n",
    "# Chemin pour accéder aux enregistrements du dataset\n",
    "dataset_path = f\"datasets/r/{dataset_sha1}?dataset={dataset_id}\"\n",
    "\n",
    "# Construction de l'URL complète\n",
    "url = f\"{base_url}{dataset_path}\"\n",
    "\n",
    "# Exécution de la requête GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérification si la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Lecture du contenu CSV\n",
    "    data_communes = pd.read_csv(StringIO(response.content.decode('utf-8')))\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439cde19-5119-4f8b-ad43-4aa52a258015",
   "metadata": {},
   "source": [
    "On enregistre la base pour ne plus avoir à éxécuter la requête API à chaque étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acbfbde4-e940-4de5-ac12-85fb9ab6f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consumption.to_csv('data_consumption.csv', index=False)\n",
    "data_communes.to_csv('data_communes.csv', index=False)\n",
    "data_weather.to_csv('data_weather.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fe2b984-6df1-4418-9d2f-0de22d555a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_consumption = pd.read_csv('data_consumption.csv')\n",
    "data_communes = pd.read_csv('data_communes.csv')\n",
    "data_weather = pd.read_csv('data_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8731d52-7614-4678-8ea2-d7986fc8708f",
   "metadata": {},
   "source": [
    "Les dataframes de consommation et météorologiques  étant sous forme de séries temporelles, on place les dates en index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64d8baac-cf50-44c6-ae2d-47dd2ebc0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consumption['date'] = pd.to_datetime(data_consumption['date'])\n",
    "data_consumption.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7236e78c-1896-4c2e-a4cc-fdaf6d9015c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code_commune_INSEE          0\n",
       "nom_commune_postal          0\n",
       "code_postal                 0\n",
       "libelle_acheminement        0\n",
       "ligne_5                 35944\n",
       "latitude                  269\n",
       "longitude                 269\n",
       "code_commune                7\n",
       "article                 36621\n",
       "nom_commune                 0\n",
       "nom_commune_complet         0\n",
       "code_departement            7\n",
       "nom_departement           267\n",
       "code_region               267\n",
       "nom_region                267\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_communes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39f82213-eb2c-47f7-87ce-91b4486a89e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "heure                                 0\n",
       "code_insee_region                     0\n",
       "region                                0\n",
       "consommation_brute_electricite_rte    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_del = pd.Index(['date_heure', 'consommation_brute_gaz_grtgaz', 'statut_grtgaz', 'consommation_brute_gaz_terega', 'statut_terega','consommation_brute_gaz_totale','statut_rte', 'consommation_brute_totale'])\n",
    "data_consumption.drop(col_del, axis=1,inplace=True)\n",
    "data_consumption.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8c7cb-badf-4d28-94e9-f72d8ece5f11",
   "metadata": {},
   "source": [
    "## Aggrégation des consommations par date et par région"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5c4c237-d1bd-4bc0-ad74-c5406abd1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consumption1 = data_consumption.groupby(['code_insee_region','date'], as_index = False)\n",
    "data_consumption2 = data_consumption.groupby(['code_insee_region','date','heure'], as_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb2cbf64-f398-43b7-a152-fd184f76c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_498/3628389353.py:1: FutureWarning: A grouping was used that is not in the columns of the DataFrame and so was excluded from the result. This grouping will be included in a future version of pandas. Add the grouping as a column of the DataFrame to silence this warning.\n",
      "  data_consumption1 = data_consumption1['consommation_brute_electricite_rte'].sum()\n",
      "/tmp/ipykernel_498/3628389353.py:2: FutureWarning: A grouping was used that is not in the columns of the DataFrame and so was excluded from the result. This grouping will be included in a future version of pandas. Add the grouping as a column of the DataFrame to silence this warning.\n",
      "  data_consumption2 = data_consumption2['consommation_brute_electricite_rte'].sum()\n"
     ]
    }
   ],
   "source": [
    "data_consumption1 = data_consumption1['consommation_brute_electricite_rte'].sum()\n",
    "data_consumption2 = data_consumption2['consommation_brute_electricite_rte'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fdc6435-19e3-499a-a0dc-45acb2a7855b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_insee_region</th>\n",
       "      <th>consommation_brute_electricite_rte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>399392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>492157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>487111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>470053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>433732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code_insee_region  consommation_brute_electricite_rte\n",
       "0                 11                              399392\n",
       "1                 11                              492157\n",
       "2                 11                              487111\n",
       "3                 11                              470053\n",
       "4                 11                              433732"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_consumption1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe6e28-4e9f-4611-9d33-50cf2cf7ae6e",
   "metadata": {},
   "source": [
    "### Traitement de la base weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b28e9f41-7ccc-4f02-afa2-0fae34b41d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouveaux_noms_colonnes={'numer_sta':'num_station', 'date':'date_UTC', 'pmer':'pression_mer', 'tend':'variation_pression_3h', 'cod_tend':'type_tendance_baro', 'dd':'direction_vent', 'ff':'vitesse_vent', 't':'temperature', 'td':'point_de_rosee',\n",
    "       'u':'humidite', 'vv':'visibilite_horizontale', 'ww':'temps_present', 'w1':'temps_passe_1', 'w2':'temps_passe_2', 'n':'nebulosite_totale', 'nbas':'nebulosite_nuage_etage_inf', 'hbas':'hauteur_base_nuage_etage_inf', 'cl':'type_nuage_etage_inf', 'cm':'type_nuage_etage_moyen', 'ch':'type_nuage_etage_sup',\n",
    "       'pres':'pression_station', 'niv_bar':'niveau_barometriq', 'geop':'geopotentiel', 'tend24':'variation_pression_24h', 'tn12':'temperature_min_sur_12h', 'tn24':'temperature_min_sur_24h', 'tx12':'temperature_max_sur_12h', 'tx24':'temperature_max_sur_24h',\n",
    "       'tminsol':'temperature_min_sol_sur_12h', 'sw':'methode_mesure_temperature_thermometre_mouille', 'tw':'temperature_thermometre_mouille', 'raf10':'rafales_10_dernieres_minutes', 'rafper':'rafales_sur_une_periode', 'per':'periode_mesure_rafales', 'etat_sol':'etat_du_sol', 'ht_neige':'hauteur_totale_couche_neige/glace_au_sol',\n",
    "       'ssfrai':'hauteur_neige_fraiche', 'perssfrai':'periode_mesure_neige_fraiche', 'rr1':'precipitation_dans_1_derniere_heure', 'rr3':'precipitation_dans_3_derniere_heure', 'rr6':'precipitation_dans_6_derniere_heure', 'rr12':'precipitation_dans_12_derniere_heure', 'rr24':'precipitation_dans_24_derniere_heure', 'phenspe1':'phenomene_special_1',\n",
    "       'phenspe2':'phenomene_special_2', 'phenspe3':'phenomene_special_3', 'phenspe4':'phenomene_special_4', 'nnuage1':'nebulosite_couche_nuageuse_1', 'ctype1':'type_de_nuage_1', 'hnuage1':'hauteur_de_base_nuage_1',\n",
    "       'nnuage2':'nebulosite_couche_nuageuse_2', 'ctype2':'type_de_nuage_2', 'hnuage2':'hauteur_de_base_nuage_2', 'nnuage3':'nebulosite_couche_nuageuse_3', 'ctype3':'type_de_nuage_3', 'hnuage3':'hauteur_de_base_nuage_3',\n",
    "       'nnuage4':'nebulosite_couche_nuageuse_4', 'ctype4':'type_de_nuage_4', 'hnuage4':'hauteur_de_base_nuage_4', 'coordonnees':'coordonnees', 'nom':'nom_commune',\n",
    "       'type_de_tendance_barometrique':'type_de_tendance_barometrique', 'temps_passe_1':'temps_passe_1', 'temps_present':'temps_present', 'tc':'temperature_degre_celcius',\n",
    "       'tn12c':'temperature_celcius_min_sur_12h', 'tn24c':'temperature_celcius_min_sur_24h', 'tx12c':'temperature_celcius_max_sur_12h', 'tx24c':'temperature_celcius_max_sur_24h', 'tminsolc':'temperature_celcius_min_sol_sur_12h', 'latitude':'latitude', 'longitude':'longitude',\n",
    "       'altitude':'altitude', 'libgeo':'libelle_geolocalisation', 'codegeo':'code_geolocalisation', 'nom_epci':'nom_EPCI', 'code_epci':'code_EPCI', 'nom_dept':'nom_departement',\n",
    "       'code_dep':'code_departement', 'nom_reg':'nom_region', 'code_reg':'code_region', 'mois_de_l_annee':'mois'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2838381c-650f-4027-9c14-a7829ab5a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_meteo_modifie_1 = data_weather.rename(columns=nouveaux_noms_colonnes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc70ba-bc90-4192-9483-95f63ee33cb4",
   "metadata": {},
   "source": [
    "## Identification des variables météoroliques à retenir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7439de-34e7-4582-abf3-20564f0d48c1",
   "metadata": {},
   "source": [
    "Il est question d'identifier les les variables météorologiques se reférant aux espects retenus Après une revue de la littérature:\n",
    "* __Température :__ les variables identifiées sont la température en degré celcius ('temperature_degre_celcius'), la température minimale sur les 24 dernières heures ('temperature_celcius_min_sur_24h'), et la température maximale sur les 24 dernières heures ('temperature_celcius_max_sur_24h')\n",
    "* __Vitesse du vent :__ la variable identifiée est la vitesse du vent ('vitesse_vent')\n",
    "* __Couverture nuageuse :__ la variable identifié est la nébulosité totale et renvoie à la quantité totale de nuages couvrant le ciel ou plus simplement le degré de couverture nuageuse dans le ciel ('nebulosite_totale')\n",
    "* __Humidité :__ la variable identifiée est l'indice d'humidité ('humidite')\n",
    "* __Rayonnement global :__ la variable identifiée pour capter cet aspect est la distance maximale à laquelle des objets peuvent être clairement discernés à l'horizon ('visibilite_horizontale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e716-2902-4346-af32-4f40eff117f3",
   "metadata": {},
   "source": [
    "### Vérification et gestion des valeurs manquantes pour les variables d'agrégation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41440d3-4b05-466c-9ca5-a37ebdda21fa",
   "metadata": {},
   "source": [
    "#### Vérification des régions et départements manquants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5322d479-654c-4151-a51c-ad077b86dedd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242582\n",
      "242582\n",
      "242582\n",
      "242582\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(base_meteo_modifie_1['nom_region'].isnull().sum())\n",
    "print(base_meteo_modifie_1['code_region'].isnull().sum())\n",
    "print(base_meteo_modifie_1['nom_departement'].isnull().sum())\n",
    "print(base_meteo_modifie_1['code_departement'].isnull().sum())\n",
    "print(base_meteo_modifie_1['nom_commune'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158c6f6-8ac9-4e41-9150-a03fd470ee2f",
   "metadata": {},
   "source": [
    "On constate que tous les noms de commune sont renseignés, mais certains noms de région et département ne le sont pas. Nous allons donc utiliser les noms de commune pour renseigner les départements et régions manquants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec43207-1a70-4f0e-af40-0b3b6871d592",
   "metadata": {},
   "source": [
    "Nous explorons la possibilité de completer les régions et départements manquants à partir de la base de données des communes, régions et départements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6885a563-6952-4929-bd7b-d65fa6f1283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352712\n",
      "2352712\n",
      "2352712\n",
      "2352712\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test=pd.merge(base_meteo_modifie_1, data_communes, on=['nom_commune'], how='left')\n",
    "print(test['nom_region_y'].isnull().sum())\n",
    "print(test['code_region_y'].isnull().sum())\n",
    "print(test['nom_departement_y'].isnull().sum())\n",
    "print(test['code_departement_y'].isnull().sum())\n",
    "print(test['nom_commune'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ae6af-60eb-4201-94a5-e41d01d87983",
   "metadata": {},
   "source": [
    "## Agrégation des variables météorologiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95393de0-00b4-4c22-b1dc-103502001ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_meteo_modifie_2=base_meteo_modifie_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c61a1-e484-4fd0-a754-7d2655038721",
   "metadata": {},
   "source": [
    "Il est ici question de construire les variables qui seront utilisées dans les analyses. Les données seront agrégées au niveau régional et à une fréquence journalière."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687ae6e-2641-486c-a890-ac073eed80cb",
   "metadata": {},
   "source": [
    "Il est ici question de construire les variables qui seront utilisées dans les analyses. Les données seront agrégées au niveau régional et à une fréquence journalière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18629c0b-23ef-4d80-83b7-430bb3764337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_sous_chaine(chaine, debut, fin):\n",
    "    \"\"\"\n",
    "    Extrait la sous-chaîne de 'debut' à 'fin' (inclus) de la chaîne donnée.\n",
    "    \"\"\"\n",
    "    if debut < 0 or fin >= len(chaine):\n",
    "        raise ValueError(\"Indices de début ou de fin invalides.\")\n",
    "    return chaine[debut : fin + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0950b6-8fd3-458d-892d-5d32a120c945",
   "metadata": {},
   "source": [
    "Nous appliquons la fonction à la base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56757649-4d1d-48ee-976d-fff8cca79ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_meteo_modifie_2['jour']=base_meteo_modifie_2['date_UTC'].apply(extraire_sous_chaine,debut=0,fin=9)\n",
    "base_meteo_modifie_2['heure']=base_meteo_modifie_2['date_UTC'].apply(extraire_sous_chaine,debut=11,fin=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637650da-bb17-4e5f-9e93-ee69654cb70f",
   "metadata": {},
   "source": [
    "## Agrégation des variables à l'échelle départementale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a767c3-427d-4cbb-9f39-0157724ef7a5",
   "metadata": {},
   "source": [
    "Tout d'abord, agrégeons les variables 'temperature_degre_celcius','vitesse_vent', 'nebulosite_totale', 'humidite', 'visibilite_horizontale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e997c0e8-383f-4828-a6c8-65816b330831",
   "metadata": {},
   "outputs": [],
   "source": [
    "agregation_dep_mean = base_meteo_modifie_2.groupby(['jour','code_departement']).agg({'temperature_degre_celcius': 'mean', 'vitesse_vent': 'mean','nebulosite_totale':'mean','visibilite_horizontale':'mean'})\n",
    "agregation_dep_mean = agregation_dep_mean.rename(columns={'jour':'jour','code_departement':'code_departement','temperature_degre_celcius':'temperature_degre_celcius_departement','vitesse_vent':'vitesse_vent_departement','nebulosite_totale':'nebulosite_totale_departement','visibilite_horizontale':'visibilite_horizontale_departement'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5805b781-f402-4751-887d-c68a5a7a2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_meteo_modifie_3 = pd.merge(base_meteo_modifie_2, agregation_dep_mean, on=['jour','code_departement'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276e6ea-735d-4803-97ea-94059f2869dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f65e1-e478-4279-adef-66474465a6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a62179-7c7b-4b3f-a7cc-dad767b4f9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9aee1-3999-4a1b-bdfd-f2ec47a609a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248a813-4f52-4171-a4ce-da96eba145da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67f3e3-d58c-4bba-b64b-caf617b3b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64b3491-dd4c-4ee0-baa2-2142b6138b54",
   "metadata": {},
   "source": [
    "Imputation des variables manquantes : KNN Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c36cc6-6e6e-4d76-8cd1-6fb3a7481e7e",
   "metadata": {},
   "source": [
    "On se dit que les zones ayant des données semblables météorologiques \"se ressemblent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fed37c-7e55-4407-abd7-bc15fef22626",
   "metadata": {},
   "source": [
    "On vérifie d'abord que toutes les variables sont numériques; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d4a63ec-fbfb-470f-9aa9-448a071fa34b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code_commune_INSEE       object\n",
       "nom_commune_postal       object\n",
       "code_postal               int64\n",
       "libelle_acheminement     object\n",
       "ligne_5                  object\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "code_commune            float64\n",
       "article                  object\n",
       "nom_commune              object\n",
       "nom_commune_complet      object\n",
       "code_departement         object\n",
       "nom_departement          object\n",
       "code_region             float64\n",
       "nom_region               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cas des données de consommation\n",
    "data_consumption.dtypes\n",
    "\n",
    "# Cas des données des communes\n",
    "data_communes.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30dd3c-d6c0-4dc1-a336-6be41dd032eb",
   "metadata": {},
   "source": [
    "On convertit les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a3341-7766-4daa-aaf0-3b972ffb2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23535434-82b4-4bc5-a707-0bdd5cb816dd",
   "metadata": {},
   "source": [
    "On applique ensuite le KNN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77e5d418-f8c2-4d8a-b4ea-a558cc30efda",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2019-02-01T06:00:00+01:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_498/1320799942.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcons_imputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcom_imputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Appliquer l'imputation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_consumption_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcons_imputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_consumption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_consumption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdata_communes_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom_imputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_communes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_communes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             return_tuple = (\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1149\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/impute/_knn.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mforce_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mforce_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    912\u001b[0m                         )\n\u001b[1;32m    913\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m                 raise ValueError(\n\u001b[1;32m    918\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 ) from complex_warning\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_latex_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \"\"\"\n\u001b[0;32m-> 2084\u001b[0;31m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mLaTeX\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparticular\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m         \u001b[0mMainly\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnbconvert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjupyter\u001b[0m \u001b[0mnotebook\u001b[0m \u001b[0mconversion\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \"\"\"\n\u001b[1;32m   2087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"styler.render.repr\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"latex\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2019-02-01T06:00:00+01:00'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialiser l'imputeur KNN\n",
    "cons_imputer = KNNImputer(n_neighbors=2)\n",
    "com_imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Appliquer l'imputation\n",
    "data_consumption_imputed = pd.DataFrame(cons_imputer.fit_transform(data_consumption), columns=data_consumption.columns)\n",
    "data_communes_imputed = pd.DataFrame(com_imputer.fit_transform(data_communes), columns = data_communes.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182511a-29be-416e-bce0-4c377b1305dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36f08d-b4e9-47fa-ba55-bc692e04f8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85a09d17-c807-4647-8552-d12a82da5136",
   "metadata": {},
   "source": [
    "### Traitement de la base geographique pour le calcul des surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e95146-0101-4d35-b608-8b88086d946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e5367-ab3f-429a-a466-6bcc9a477be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On recupère les colonnes importantes de la base de données\n",
    "cols = ['geo_point_2d', 'reg_code','reg_name', 'dep_code', 'dep_name', 'com_code', 'com_name', 'geometry']\n",
    "data_geo2 = data_geo[cols]\n",
    "cols_to_mod = ['reg_code','reg_name', 'dep_code', 'dep_name', 'com_code', 'com_name']\n",
    "#On effectue un nettoyage des données contenues dans les colonnes\n",
    "data_geo2[cols_to_mod] = data_geo2[cols_to_mod].applymap(lambda x: ''.join(filter(lambda char: char.isalnum() or char in ['_', '-', '@', '#'], str(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87075d-3dce-4b78-bcae-f908bcb89323",
   "metadata": {},
   "source": [
    "### Calcul de la superficie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4a3c3-95f4-4d3b-a362-cc6af676b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On divise par par 1e6 pour obtenir la superficie par commune en km²\n",
    "data_geo2['superficie_comm'] = data_geo2['geometry'].area / 1e6  \n",
    "\n",
    "#On fait un regroupement par département, et on calcule la superficie par département en sommant les autres superficies\n",
    "data_geo2['superficie_dep'] = data_geo2.groupby('dep_code')['superficie_comm'].transform('sum')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
